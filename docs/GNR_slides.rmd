---
title: "GNR"
subtitle: 'A. Gandhi, S. Navarro, and D. A. Rivers. "On the identification of gross output production functions". *Journal of Political Economy*, 128.8 (2020).'
author: "Seiro Ito"
date: "`r format(Sys.time(), '%Y年%m月%d日 %R')`"
header_includes:
    - \usepackage{tikz}
    - \usepackage{adjustbox}
    - \usepackage{seiro_minimal}
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, default-fonts, xaringan-themer.css]
    nature:
      highlightStyle: github
      countIncrementalSlides: false
      ratio: "16:9"
#    includes:
#      before_body: mathabbreviation.html
    citation_package: natbib
bibliography: c:/seiro/settings/Tex/seiro.bib
self_contained: true
# setwd(path <- "c:/data/MinWageMarketPower/"); rmarkdown::render("docs/GNR_slides.Rmd")
# renderthis::to_pdf("docs/GNR_slides.Rmd")
# git init
# git add /docs    ## this adds a folder to track
# git commit -am "initial commit"
# git config --global user.name "SeiroIto"
# git remote add origin https://github.com/SeiroIto/MinWageMarketPower.git  ## specify remote
# git push -f origin main  ## push files to remote
---
class: inverse
```{r xaringan setup, include=FALSE, warning=FALSE}
xaringanExtra::use_panelset()
library(xaringanthemer)
#style_mono_dark(base_color = "#cbf7ed")
#style_mono_light(base_color = "#23395b")
style_solarized_light(
  text_color = "black", 
  title_slide_background_color = "darkblue",
  link_color = "green")
# xaringancolor setup
library(xaringancolor)
xaringancolor::setup_colors(
  red ="red",
  green = "green",
  blue = "blue"
)
library(RefManageR)
```
```{r xaringan bib, include=FALSE, cache=T}
# bibliography setup
BibOptions(check.entries = FALSE, style = "markdown", cite.style = "authoryear",
  bib.style = "authoryear", dashed = T, hyperlink = "to.doc")
## (old) Uncomment below for the 1st run of each R session. 2nd run will get cached results.
## (new) Delete cache if you add an entry to seiro.bib.
myBib <- ReadBib("c:/seiro/settings/TeX/seiro.bib")
```
<div style = "position:fixed; visibility: hidden">
$$\require{color}\definecolor{red}{rgb}{1, 0, 0}$$
$$\require{color}\definecolor{green}{rgb}{0, 1, 0}$$
$$\require{color}\definecolor{blue}{rgb}{0, 0, 1}$$
</div>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      red: ["{\\color{red}{#1}}", 1],
      green: ["{\\color{green}{#1}}", 1],
      blue: ["{\\color{blue}{#1}}", 1]
    },
    loader: {load: ['[tex]/color']},
    tex: {packages: {'[+]': ['color']}}
  }
});
</script>
<style>
.red {color: #FF0000;}
.green {color: #00FF00;}
.blue {color: #0000FF;}
</style>
```{r flair_color, echo=FALSE}
library(flair)
red <- "#FF0000"
green <- "#00FF00"
blue <- "#0000FF"
```
```{css echo=FALSE}
.highlight-last-item > ul > li,
.highlight-last-item > ol > li {
  opacity: 0.5;
}
.highlight-last-item > ul > li:last-of-type,
.highlight-last-item > ol > li:last-of-type {
  opacity: 1;
}
.inverse {
  background-color: #272822;
  color: #d6d6d6;
  text-shadow: 0 0 20px #333;
}
mark.red {
    color:#ff0000;
    background: none;
}
mark.blue {
    color:#0000A0;
    background: none;
}
.my-style {
  font-weight: bold;
  font-style: italic;
  font-size: 1.5em;
  color: red;
}
blockquote {
  border-left: .2px solid #275d38;
  margin: -5px 80px -5px 20px;
  padding-top: -0.5px;
  padding-bottom: -0.5px;
  line-height: 1.35em;
}
```

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      E: "{\\Large\\varepsilon}",
      bfx: "{\\mathbf{x}}",
      bfX: "{\\mathbf{X}}",
      bfbeta: "{\\boldsymbol{\\beta}}",
      st: "{\\mbox{s.t.}}"
    }
  }
});
</script>

## Gross output production function parameters are not indentified. 

--

## When using profit maximization FOC in material demand inversion and using lagged inputs as IVs to estimate other parameters.  

--

## Exogeonous material demand variations (say, by input prices which may or maynot be observed) can help identification but weak.

`r Citet(myBib, "GNR2020")`


---
class: inverse

### Then, can't we just use a VA production function instead?

--

> the conditions which a value added production function can straightforwardly be derived from a more primitive gross output production function are quite restrictive.  

<p style="margin-bottom:2cm;">.pull-left[] .pull-right[`r Citet(myBib, "DeLoeckerSyverson2021", after = ", 5.6.2")`]</p>

--

Taking out VA from Y=VA(K, L)+M form 

* Gives a .red[biased estimate] when regressing $VA$ on $K, L$, except  under `r Citep(myBib, "BasuFernald1997", after = ", p.255")`  

--

  * Zero markup in output markets, or,  
  * Zero substitution between $(K, L)$ and $M$ (Leontief-in-materials).  

--

* Does not admit contribution of $M$ to TFP growth.  

--

  * At odds with a growing recognition on the role of imported intermediate goods on TFP.  

---
class: inverse

### Macro TFP literature uses value-added production functions. 

--

### Macro misallocation, IO, and trade literature use gross output production functions. 

* Imported goods on TFP.  
* Separating demand side heterogeneity (shocks, markups) from TFP.  
* `r Citet(myBib, "AckerbergCavesFrazer2015")` considers identification of VA production with material input inversion (conditional on $l_{jt}$) = gross output production function with Leontief in materials. Not general gross output production functions.

--

### Dev econ uses VA production as household production functions (ag HH model, etc.) with time-invariant shocks $\omega_{j}$.

* Effectively hid the problem under the rug, using FEs looked fine.  
* HH data often include input/output $p$ and $q$ separately, rendering IV+FE use.  
* Could have estimated VA+LIM or GO production functions with $\omega_{jt}$.  


---
### Why unidentified?

* Given $\omega_{jt-1}, k_{jt}$, $m_{jt-1}(\omega_{jt-1}, \epsilon_{jt-1})$ is uncorrelated with $m_{jt}(\omega_{jt-1}+\xi_{jt}, \epsilon_{jt})$, so cannot be used as an IV.  
* Oversimplifying, but, when $f(\bfx_{jt})+h(\bfx_{i, t-1}, d_{t-1})$ is on RHS, it is hard to separately identify $f$ and $h$ with $\bfx_{i,t-1}$ as IVs. 

* Substitution of $\omega_{jt}$ using material demand (proxy variable) or AR(1) (dynamic panel) suffers.  

---
### What does it mean if we want to estimate wage markdown using financial statement data?
 
--

* It means we must do it with VA or VA+Leontief-in-materials `r Citep(myBib, c("AckerbergCavesFrazer2015", "Rubens2023"))`.   

  * `r Citet(myBib, "Rubens2023")` derives an estimable markdown formula for a monopsonist.  
  * `r Citet(myBib, c("Bruno1978", "Diewert1978"))`: Without ex-post shocks $\epsilon_{jt}$, VA elasticity $\simeq \frac{\textrm{VA}}{\textrm{GO}}$ * gross output elastictiy.  
  * `r Citet(myBib, "GNR2017")`: With ex-post shocks $\epsilon_{jt}$, VA elasticity $\neq \frac{\textrm{VA}}{\textrm{GO}}$ * gross output elastictiy, except...  
    * if gross output is LIM and material input enters linearly $\min\{\mathcal H(K_{jt}, L_{jt}), aM_{jt}\}$, one can estimate VA production function with $Y_{jt}=\mathcal H(K_{jt}, L_{jt})e^{\omega_{jt}+\epsilon_{jt}}$.  

--

* Other cases: When are materials substitutable with capital and labour? Energy. But not coffee beans for coffee. 

--

* VA or VA+LIM can be good enough in measuring wage markdown `r Citep(myBib, "HashemiKirovTraina2022")`.  

---
class: middle, center, inverse

# Issue: Transmission/endogeneity problem

---
class: inverse
### Estimate production function parameters under Hicks-neutral unobservable heterogenous productivity $\omega_{j}$?

\begin{equation*}
y_{j}=f(K_{j}, L_{j}, \omega_{j})+\epsilon_{j}=f(K_{j}, L_{j})+\omega_{j}+\epsilon_{j}. 
\end{equation*}

--

### OLS estimates are biased: "Transmission problem".

--

* There remains a separate bias of omitted prices `r Citep(myBib, "DeLoeckerGoldberg2014")`

--

### Use FE. But what if $\omega_{j}=\omega_{jt}$? FE does not solve the problem.

--

### Solution: Lagged input decision (lagged input variable) is orthogonal to current shocks. IV.  

* Dynamic panel estimators.  
* Proxy variable methods.  
---
class: inverse
### GNR: 1. Lagged input = IV does not nonparametrically identify parameters.  

--

### GNR: 2. Use of price-taker profit max FOC achieves identification.  

--

### (Price-taker profit max is restrictive.)
---
class: inverse, center, middle

# Negative result: Nonparametric nonidentification

---
class: inverse

### $y_{jt}=f(\bfx_{jt})+\omega_{jt}+\epsilon_{jt}$

--

### Substitute unobservable productivity $\omega_{jt}$ with $h(\bfx_{jt-1},d_{t-1})$, real factor price $d_{t-1}$.

--

### If $d_{t}$ is constant, conditional (on IV, usually $\bfx_{jt-1}$) expectation of $y_{jt}$ cannot separately identify parameters of $f, h$.
--

### This also holds for dynamic panel estimators. 

--

### Monte Carlo: Even when $d_{t}$ varies, identification is weak, not usable.
---
class: inverse
Production function

<!--$$
Y_{jt}=F^{0}(K_{jt}, L_{jt}, M_{jt})e^{\omega_{jt}+\epsilon_{jt}}.
$$-->

\begin{aligned}
Y_{jt}
&=F^{0}(K_{jt}, L_{jt}, M_{jt})e^{\omega_{jt}+\epsilon_{jt}},\\
y_{jt}
&=
f^{0}\left(\bfx_{jt}\right)+\omega_{jt}+\epsilon_{jt},\\
&=
f^{0}\left(\bfx_{jt}\right)-f^{0}_{M}\left(\bfx_{jt}\right)-\E[\epsilon_{jt}]+d_{t}+\epsilon_{jt}.
\end{aligned}

where we used price-taking expected profit maximization FOC

\begin{equation*}
\max_{M_{jt}}
\;\;\; \E\left[P_{jt}F^{0}(K_{jt}, L_{jt}, M_{jt})e^{\omega_{jt}+\epsilon_{jt}}\right]-\rho_{jt}M_{jt}
\end{equation*}

\begin{equation*}
\frac{\partial F^{0}\left(\bfx_{jt}\right)e^{\omega_{jt}}\E[e^{\epsilon_{jt}}]}{\partial M_{jt}}=\frac{\rho_{jt}}{P_{jt}}.
\end{equation*}

\begin{equation*}
f^{0}_{M}\left(\bfx_{jt}\right)+\omega_{jt}+\E[\epsilon_{jt}]=\underbrace{\ln\rho_{t}-p_{t}}_{\equiv d_{t}},
\quad 
f^{0}_{M}\left(\bfx_{jt}\right)=\ln\frac{\partial F^{0}\left(\bfx_{jt}\right)}{\partial M_{jt}},
\end{equation*}
so one can solve for $\omega_{jt}$ ("invert")
\begin{equation*}
\omega_{jt}
=
-f^{0}_{M}\left(\bfx_{jt}\right)-\E[\epsilon_{jt}]+d_{t}.
\end{equation*}
---
class: inverse
\begin{aligned}
\omega_{jt}
&=
\underbrace{-f^{0}_{M}\left(\bfx_{jt}\right)-\E[\epsilon_{jt}]}_{\equiv M^{-1}\left(\bfx_{jt}\right)}+d_{t},\\
y_{jt}
&=
f^{0}\left(\bfx_{jt}\right)+\omega_{jt}+\epsilon_{jt},\\
&=
\underbrace{f^{0}\left(\bfx_{jt}\right)+M^{-1}\left(\bfx_{jt}\right)}_{\equiv \phi_{G}\left(\bfx_{jt}\right)}+d_{t}+\epsilon_{jt}.\quad (\phi_{G}+d_{t}=\phi \ \mathrm{of\ DLW})
\end{aligned}

Using $\phi_{G}$
\begin{aligned}
\omega_{jt}
&=
f^{0}\left(\bfx_{jt}\right)+M^{-1}\left(\bfx_{jt}\right)+d_{t}-f^{0}\left(\bfx_{jt}\right),\\
&=
\phi_{G}\left(\bfx_{jt}\right)+d_{t}-f^{0}\left(\bfx_{jt}\right),\\
\omega_{jt-1}
&=
\phi_{G}\left(\bfx_{jt-1}\right)+d_{t-1}-f^{0}\left(\bfx_{jt-1}\right).
\end{aligned}

First-order Markov ass. gives
\begin{aligned}
\omega_{jt}
&=
h^{0}\left(\omega_{jt-1}\right)+\xi_{jt}, \quad (h^{0}=g \ \mathrm{of\ DLW})\\
&=
h^{0}\left\{\phi_{G}\left(\bfx_{jt-1}\right)+d_{t-1}-f^{0}\left(\bfx_{jt-1}\right)\right\}+\xi_{jt}.
\end{aligned}
So, with price-taking profit max FOC and FO Markov, we get
\begin{equation*}
y_{jt}=f^{0}\left(\bfx_{jt}\right)+h^{0}\left\{\phi_{G}\left(\bfx_{jt-1}\right)+d_{t-1}-f^{0}\left(\bfx_{jt-1}\right)\right\}+\xi_{jt}+\epsilon_{jt}.
\end{equation*}
---
class: inverse
Theorem 1: Nonidentification of parameters
### If real factor price $d_{t}=d$ for all $t$,  model+data $\rightarrow$ non unique parameter sets

* Conditional expectation (conditioned on IVs $\Gamma_{jt}$) of $y_{jt}=f^{0}(\bfx_{jt})+h^{0}(z)+\xi_{jt}+\epsilon_{jt}$ can be expressed as $\E\left[f^{0}(\bfx_{jt})\left|\right.\Gamma_{jt}\right]+\E\left[h^{0}(z)\left|\right.\Gamma_{jt}\right]$.  
* But, when we define new functions $\tilde{f}\neq f^{0}, \tilde{h}\neq h^{0}$ by mixing a nuisance function at a constant proportion $a$ for any $a\in(0, 1)$ as in the below, we can show (in Theorem 1) that their conditional (on IVs) expectations are same $\E\left[f^{0}(\bfx_{jt})\left|\right.\Gamma_{jt}\right]+\E\left[h^{0}(z)\left|\right.\Gamma_{jt}\right]=\E\left[\tilde{f}(\bfx_{jt})\left|\right.\Gamma_{jt}\right]+\E\left[\tilde{h}(z)\left|\right.\Gamma_{jt}\right]$.  

\begin{aligned}
\tilde{f}\left(\bfx_{jt}\right)&\equiv (1-a)f^{0}\left(\bfx_{jt}\right)+a\phi_{G}\left(\bfx_{jt}\right),\\
\tilde{h}\left(z\right)&\equiv ad+(1-a)h^{0}\left(\frac{z-ad}{1-a}\right).
\end{aligned}

* For true functions $f^{0}, h^{0}$, there are infinitely many *wrong* functions $\tilde{f}, \tilde{h}$, or different "parameters", whose conditional expectations (of functional values) are identical (*observationally equivalent*). 
---
class: inverse
Start from here:
\begin{equation*}
y_{jt}=f^{0}\left(\bfx_{jt}\right)+h^{0}\left\{\phi_{G}\left(\bfx_{jt-1}\right)+d_{t-1}-f^{0}\left(\bfx_{jt-1}\right)\right\}+\xi_{jt}+\epsilon_{jt}.
\end{equation*}
Expectation conditional on IVs $\Gamma_{jt}=\left(k_{jt}, l_{jt}, \bfx_{jt-1}, d_{t-1}, y_{jt-1}, \cdots\right)$: 
\begin{equation*}
\E\left[y_{jt}|\Gamma_{jt}\right]=\E\left[f^{0}\left(\bfx_{jt}\right)|\Gamma_{jt}\right]+h^{0}\left\{\phi_{G}\left(\bfx_{jt-1}\right)+d_{t-1}-f^{0}\left(\bfx_{jt-1}\right)\right\}.
\end{equation*}
Note from the definition of $\phi_{G}$, we have:
\begin{equation*}
\E\left[y_{jt}|\Gamma_{jt}\right]=\E\left[\phi_{G}\left(\bfx_{jt}\right)+d_{t}|\Gamma_{jt}\right].
\end{equation*}
Substitute this RHS into LHS of the previous equation, and rearrange:
\begin{equation}
\E\left[\phi_{G}\left(\bfx_{jt}\right)+d_{t}-f^{0}\left(\bfx_{jt}\right)|\Gamma_{jt}\right]=h^{0}\left\{\phi_{G}\left(\bfx_{jt-1}\right)+d_{t-1}-f^{0}\left(\bfx_{jt-1}\right)\right\}.
\tag{10}
\end{equation}
---
class: inverse
Define $\tilde{f}, \tilde{h}$ and form systematic part of $\tilde{y}_{jt}$:
\begin{aligned}
\tilde{f}\left(\bfx_{jt}\right)&+\tilde{h}\left\{\phi_{G}\left(\bfx_{jt-1}\right)+d_{t-1}-\tilde{f}\left(\bfx_{jt-1}\right)\right\}\\
&=
(1-a)f^{0}\left(\bfx_{jt}\right)+a\phi_{G}\left(\bfx_{jt}\right)+ad+(1-a)h^{0}\left\{\frac{\phi_{G}\left(\bfx_{jt-1}\right)+d-\tilde{f}\left(\bfx_{jt-1}\right)-ad}{1-a}\right\}.
\end{aligned}
Numerator of square bracket
\begin{equation*}
\phi_{G}\left(\bfx_{jt-1}\right)+d-(1-a)f^{0}\left(\bfx_{jt-1}\right)-a\phi_{G}\left(\bfx_{jt-1}\right)-ad
=
(1-a)\left\{\phi_{G}\left(\bfx_{jt-1}\right)+d-\tilde{f}\left(\bfx_{jt-1}\right)\right\}.
\end{equation*}
So
\begin{equation*}
RHS
=
f^{0}\left(\bfx_{jt}\right)+a\left\{\phi_{G}\left(\bfx_{jt}\right)+d-f^{0}\left(\bfx_{jt}\right)\right\}
+(1-a)h^{0}\left\{\phi_{G}\left(\bfx_{jt-1}\right)+d-f^{0}\left(\bfx_{jt-1}\right)\right\}.
\end{equation*}
This gives the equivalence result.
\begin{aligned}
\E&\left[\tilde{f}\left(\bfx_{jt}\right)|\Gamma_{jt}\right]+\tilde{h}\left\{\phi_{G}\left(\bfx_{jt-1}\right)+d_{t-1}-\tilde{f}\left(\bfx_{jt-1}\right)\right\}\\
&\hspace{1em}=
\E\left[f^{0}\left(\bfx_{jt}\right)|\Gamma_{jt}\right]+ah^{0}\left\{\phi_{G}\left(\bfx_{jt}\right)+d-f^{0}\left(\bfx_{jt}\right)\right\}
+(1-a)h^{0}\left\{\phi_{G}\left(\bfx_{jt-1}\right)+d-f^{0}\left(\bfx_{jt-1}\right)\right\},\\
&\hspace{1em}=
\E\left[f^{0}\left(\bfx_{jt}\right)|\Gamma_{jt}\right]+h^{0}\left\{\phi_{G}\left(\bfx_{jt}\right)+d-f^{0}\left(\bfx_{jt}\right)\right\}.
\end{aligned}
---
class: inverse, center, middle

# Semi positive result: Nonparametric identification with profit max FOC

---
class: inverse

Price-taking profit maximization gives
\begin{equation*}
P_{t}\frac{\partial F^{0}\left(\bfx_{jt}\right)}{\partial M_{jt}}e^{\omega_{jt}}\E[e^{\epsilon_{jt}}]=\rho_{t}.
\end{equation*}
Noting
\begin{equation*}
\frac{\partial \ln F^{0}\left(\bfx_{jt}\right)}{\partial m_{jt}}=
\frac{\partial \ln F^{0}\left(\bfx_{jt}\right)}{\partial F^{0}}
\frac{\partial F^{0}}{\partial M_{jt}}
\frac{\partial M_{jt}}{\partial m_{jt}}=\frac{1}{F^{0}}F^{0}_{M}M_{jt},
\end{equation*}
where the last derivative is given by:
\begin{equation*}
1=\frac{\partial M}{\partial M}=\frac{\partial M}{\partial m}\frac{\partial m}{\partial M}=\frac{\partial M}{\partial m}\frac{1}{M} \quad \Rightarrow \quad \frac{\partial M}{\partial m}=M,
\end{equation*}
so, 
\begin{equation*}
F^{0}_{M}=\frac{\partial \ln F^{0}\left(\bfx_{jt}\right)}{\partial m_{jt}}\frac{F^{0}}{M_{jt}}=f^{0}_{m}\left(\bfx_{jt}\right)\frac{F^{0}}{M_{jt}}, \quad f^{0}_{m}\left(\bfx_{jt}\right)\equiv\frac{\partial \ln F^{0}\left(\bfx_{jt}\right)}{\partial m_{jt}}.
\end{equation*}
Plugging this in gives:
\begin{aligned}
P_{t}f^{0}_{m}F^{0}\left(\bfx_{jt}\right)e^{\omega_{jt}}\E[e^{\epsilon_{jt}}]&=\rho_{t}M_{jt}.
\end{aligned}
---
class: inverse
Plugging this in gives:
\begin{aligned}
P_{t}f^{0}_{m}F^{0}\left(\bfx_{jt}\right)e^{\omega_{jt}}\E[e^{\epsilon_{jt}}]&=\rho_{t}M_{jt},\\
P_{t}f^{0}_{m}\underbrace{F^{0}\left(\bfx_{jt}\right)e^{\omega_{jt}}e^{\epsilon_{jt}}}_{\equiv Y_{jt}}e^{-\epsilon_{jt}}\E[e^{\epsilon_{jt}}]=\rho_{t}M_{jt},\\
f^{0}_{m}e^{-\epsilon_{jt}}\E[e^{\epsilon_{jt}}]=\frac{\rho_{t}M_{jt}}{P_{jt}Y_{jt}}\equiv S_{jt}.
\end{aligned}
Taking logs and denoting $\ln E[e^{\epsilon_{jt}}]=\ln\E$ gives:
\begin{equation*}
s_{jt}=\underbrace{\ln f^{0}_{m}+\ln\varepsilon}_{\equiv \ln D^{\epsilon}\left(\bfx_{jt}\right)}-\epsilon_{jt}, \quad D^{\epsilon}\left(\bfx_{jt}\right)=f^{0}_{m}\left(\bfx_{jt}\right)\E[e^{\epsilon_{jt}}].
\end{equation*}
Equivalently, *share regression* is derived:
\begin{equation*}
s_{jt}=\ln D^{\epsilon}\left(\bfx_{jt}\right)+u^{\epsilon}_{jt}.
\tag{s}\label{sharereg}
\end{equation*}

---
class: inverse
### Thereom 2: One can recover $\hat{f}_{m}$ by using the share regression.

Given $\omega_{jt}, \bfx_{jt}\in\mathcal I_{jt}$ we have $\E[\epsilon_{jt}|\mathcal I_{jt}]=0$, so regressing $s_{jt}$ on a flexible function of $\bfx_{jt}$ identifies $f^{0}_{m}$ as a set of coefficients on the part of $m_{jt}$. 
\begin{equation*}
\E\left.\left[s_{jt}\right|\bfx_{jt}\right]=\ln D^{\epsilon}\left(\bfx_{jt}\right).
\end{equation*}
This identifies $D^{\epsilon}$. This gives $\epsilon_{jt}$ and $\E=\E[e^{\epsilon_{jt}}]$:
\begin{equation*}
\hat{\epsilon}_{jt}=\ln \hat{D}^{\epsilon}-s_{jt}, \quad \hat{\E}=\E\left[e^{\ln \hat{D}^{\epsilon}-s_{jt}}\right].
\end{equation*}
This gives:
\begin{aligned}
\ln \hat{f}_{m}
&=\ln \hat{D}^{\epsilon}-\ln\hat{\E},\\
\hat{f}_{m}
&=\frac{\hat{D}^{\epsilon}}{\hat{\E}}.
\end{aligned}

---
class: inverse
### Thereom 3: Rest of $f$ is recovered up to a constant if $\hat{f}_{m}$ is known.

By the fundamental theorem of calculus:
\begin{equation}
\int f^{0}_{m}\left(\bfx_{jt}\right)dm_{jt}=f^{0}\left(\bfx_{jt}\right)+\mathcal C(k_{jt}, l_{jt}), \quad \mathrm{or}\quad
f^{0}\left(\bfx_{jt}\right)=\int f^{0}_{m}\left(\bfx_{jt}\right)dm_{jt}-\mathcal C(k_{jt}, l_{jt}).
\tag{f}\label{f0identified}
\end{equation}
where the constant of integration $\mathcal C$ is allowed to depend on $k_{jt}, l_{jt}$, or $\mathcal C=c_{0}+c_{1}(k_{jt}, l_{jt})$ without loss of generality. 

### If we can compute $c_{1}(k_{jt}, l_{jt})$, then we can identify $f^{0}\left(\bfx_{jt}\right)$ up to a constant $c_{0}$. 

---
class: inverse
Start with the production function in logs:
\begin{aligned}
\omega_{jt}
&=
y_{jt}-f^{0}-\epsilon_{jt},\\
&=
y_{jt}-\left\{\int f^{0}_{m}\left(\bfx_{jt}\right)dm_{jt}-\mathcal C(k_{jt}, l_{jt})\right\}-\epsilon_{jt}.
\end{aligned}
Define
\begin{aligned}
\green{\mathcal Y_{jt}}
&\equiv 
y_{jt}-\int \red{f^{0}_{m}\left(\bfx_{jt}\right)}dm_{jt}-\red{\epsilon_{jt}},\\
&=
\omega_{jt}-\mathcal C(k_{jt}, l_{jt}).
\end{aligned}
We already .red[have] $\red{\hat{\epsilon}_{jt}}$ and $\red{\hat{f}^{0}_{m}}$, then we can .green[get] $\green{\hat{\mathcal Y}_{jt}}$. 

Using first-order Markov on $\omega_{jt}$
\begin{aligned}
\omega_{jt}
&=
\mathcal Y_{jt}+\mathcal C(k_{jt}, l_{jt}),\\
\omega_{jt-1}
&=
\mathcal Y_{jt-1}+\mathcal C(k_{jt-1}, l_{jt-1}).
\end{aligned}
Then
\begin{aligned}
\mathcal Y_{jt}
&=
h^{0}\left\{\omega_{jt-1}\right\}+\xi_{jt}-\mathcal C(k_{jt}, l_{jt}),\\
&=
h^{0}\left\{\mathcal Y_{jt-1}+\mathcal C(k_{jt-1}, l_{jt-1})\right\}+\xi_{jt}-\mathcal C(k_{jt}, l_{jt}).
\end{aligned}
---
class: inverse
Denote $\Gamma_{y}=(k_{jt}, l_{jt}, k_{jt-1}, l_{jt-1}, \mathcal Y_{jt-1})$. Then
\begin{equation*}
E\left.\left[\mathcal Y_{jt}\right|\Gamma_{y}\right]=-\mathcal C(k_{jt}, l_{jt})+h^{0}\left\{\mathcal Y_{jt-1}+\mathcal C(k_{jt-1}, l_{jt-1})\right\}.
\end{equation*}

### Partial derivatives $c_{1}(k_{jt}, l_{jt})$ of integration constant $\mathcal C$ are (negative of) coefficients on $k_{jt}, l_{jt}$ conditional on $\mathcal Y_{jt-1}, k_{jt-1}, l_{jt-1}$.  

* We cannot identify $c_{0}$ part.  
* Using this in \eqref{f0identified} gives $\hat{f}^{0}$ up to a constant.  

### $\hat{\omega}_{jt}-c_{0}=\hat{\mathcal Y}_{jt}+\hat{c}_{1}(k_{jt}, l_{jt})$ is estimable. One can measure productivity differences $\hat{\omega}_{jt}-\hat{\omega}_{i't}$ or $\hat{\omega}_{jt}-\hat{\omega}_{it'}$. 

---
class: inverse

Cool but *semi* positive results. 

Monopolist profit maximization
\begin{aligned}
\rho_{t}
&=
\left(\red{P'_{t}F^{0}\left(\bfx_{jt}\right)}+P_{t}\frac{\partial F^{0}\left(\bfx_{jt}\right)}{\partial M_{jt}}\right)e^{\omega_{jt}}\E[e^{\epsilon_{jt}}],\\
&=
\left(\frac{\partial P}{\partial Y_{jt}}\frac{F^{0}\left(\bfx_{jt}\right)}{P_{t}}+f^{0}_{m}\left(\bfx_{jt}\right)\frac{F^{0}}{M_{jt}}\right)P_{t}e^{\omega_{jt}}\E[e^{\epsilon_{jt}}],\\
&=
\left(-\eta^{-1}(\bfx_{jt}, \omega_{jt}, \epsilon_{jt})+f^{0}_{m}\left(\bfx_{jt}\right)\frac{F^{0}e^{\omega_{jt}}\E[e^{\epsilon_{jt}}]}{M_{jt}}\right)P_{t},\\
S_{jt}
&=
-\eta^{-1}(\bfx_{jt}, \omega_{jt}, \epsilon_{jt})\frac{M_{jt}}{Y_{jt}}+f^{0}_{m}\left(\bfx_{jt}\right),\\
&=
-\frac{\eta^{-1}(\red{\bfx_{jt}}, \omega_{jt}, \epsilon_{jt})}{\beta^{M}}+f^{0}_{m}\left(\red{\bfx_{jt}}\right), \quad \beta^{M}\equiv\frac{M_{jt}}{Y_{jt}}.\\
\end{aligned}

$\red{\bfx_{jt}}$ appears both in $\eta$ and $f^{0}_{m}$ and the two functions cannot be separately nonparametrically identified with the share regression. 

* If we assume an iso-elastic demand function for the output, then elasticity does not depend on $\bfx_{jt}$ and can recover production function parameters. GNR does so in the appendix. 

---
class: inverse, center, middle

# Extensions and empirical exercises

---
class: inverse

* 
`r Citet(myBib, "DoraszelskiJaumandreu2013")` achieves identification of Cobb-Douglas using FOCs. `r Citet(myBib, "DoraszelskiJaumandreu2018")` extends it to non-Hicks neutral, CES production.  
* GNR can incoporate firm fixed effects but at the cost of loss of efficiency due to differencing.  

### Using Chile and Columbia data, OLS estimates are overestimated than GNR estimates in more flexible inputs (materials).  
### Less flexible input elasticities are underestimated with OLS. 
### Productivity is more dispersed under GNR estimates. 

---
class: inverse



```{r bib, include=FALSE}
# create a bib file for the R packages used in this document
knitr::write_bib(x = "rmarkdown", file = 'GNR_slides/GNR_slides.bib')
```
```{r results = "asis", echo = FALSE}
PrintBibliography(myBib, .opts = list(check.entries = T, sorting = "nyt",
  no.print.fields=c("eprint", "URL")),
  start = 1, end = 8)
```
---
class: inverse
```{css, eval = F, echo = F}
.remark-slide-content {
  font-size:          12px;
  line-height:        1.2em;
}
```

```{r results = "asis", echo = FALSE, warning = F}
PrintBibliography(myBib, .opts = list(check.entries = FALSE, sorting = "nyt",
  no.print.fields=c("eprint", "URL")),
  start = 9, end = 12)
```
