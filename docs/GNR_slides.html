<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>GNR</title>
    <meta charset="utf-8" />
    <meta name="author" content="Seiro Ito" />
    <script src="libs/header-attrs-2.25/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link href="libs/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="libs/panelset-0.2.6/panelset.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# GNR
]
.author[
### Seiro Ito
]
.date[
### 2024年02月02日 12:43
]

---

class: inverse


&lt;div style = "position:fixed; visibility: hidden"&gt;
`$$\require{color}\definecolor{red}{rgb}{1, 0, 0}$$`
`$$\require{color}\definecolor{green}{rgb}{0, 1, 0}$$`
`$$\require{color}\definecolor{blue}{rgb}{0, 0, 1}$$`
&lt;/div&gt;
&lt;script type="text/x-mathjax-config"&gt;
MathJax.Hub.Config({
  TeX: {
    Macros: {
      red: ["{\\color{red}{#1}}", 1],
      green: ["{\\color{green}{#1}}", 1],
      blue: ["{\\color{blue}{#1}}", 1]
    },
    loader: {load: ['[tex]/color']},
    tex: {packages: {'[+]': ['color']}}
  }
});
&lt;/script&gt;
&lt;style&gt;
.red {color: #FF0000;}
.green {color: #00FF00;}
.blue {color: #0000FF;}
&lt;/style&gt;

&lt;style type="text/css"&gt;
.highlight-last-item &gt; ul &gt; li,
.highlight-last-item &gt; ol &gt; li {
  opacity: 0.5;
}
.highlight-last-item &gt; ul &gt; li:last-of-type,
.highlight-last-item &gt; ol &gt; li:last-of-type {
  opacity: 1;
}
.inverse {
  background-color: #272822;
  color: #d6d6d6;
  text-shadow: 0 0 20px #333;
}
mark.red {
    color:#ff0000;
    background: none;
}
mark.blue {
    color:#0000A0;
    background: none;
}
.my-style {
  font-weight: bold;
  font-style: italic;
  font-size: 1.5em;
  color: red;
}
&lt;/style&gt;
&lt;script type="text/x-mathjax-config"&gt;
MathJax.Hub.Config({
  TeX: {
    Macros: {
      E: "{\\Large\\varepsilon}",
      bfx: "{\\mathbf{x}}",
      bfX: "{\\mathbf{X}}",
      bfbeta: "{\\boldsymbol{\\beta}}",
    }
  }
});
&lt;/script&gt;

## Gross output production function parameters are not indentified. 

--

## When using profit maximization FOC in material demand inversion and using IVs to estimate other parameters.  

--

## Material demand variations (say, by input prices which may or maynot be observed), can help identification but weak.

Gandhi, Navarro, and Rivers (2020)


---
### Why?

* When `\(f(\bfx_{jt})+h(\bfx_{i, t-1}, d_{t-1})\)` is on RHS, it is hard to separately identify `\(f\)` and `\(h\)` with `\(\bfx_{i,t-1}\)` as IVs. 
* Both inversion of material demand (proxy variable approach) and substitution of AR(1) productivity (dynamic panel approach) suffer.  

### What does it mean if we want to estimate wage markdown using financial statement data?


---
class: middle, inverse

# Issue: Transmission/endogeneity problem

---
class: inverse
### Estimate production function parameters under Hicks-neutral unobservable heterogenous productivity `\(A_{i}\)`?

`\begin{equation*}
Y_{i}=F(A_{i}, K, L)+e=A_{i}F(K, L)+e. 
\end{equation*}`


### OLS estimates are biased: "Transmission problem".

--

### Solution: Lagged input decision (lagged input variable) is orthogonal to current shocks. IV.  

* Dynamic panel estimators.  
* Proxy variable methods.  

--

### Past literature used value-added production functions. 

* VA production with material input inversion = gross output production function with Leontief in materials (Ackerberg, Caves, and Frazer, 2015). 

---
class: inverse
### Gross output production function = value-added production function + intermediate inputs (not in Leontief)

--

### GNR: 1. Lagged input = IV does not nonparametrically identify parameters.  

--

### GNR: 2. Use of price-taker profit max FOC achieves identification.  

--

### (Price-taker profit max is restrictive.)
---
class: inverse, middle

# Negative result: Nonparametric nonidentification

---
class: inverse

### `\(y_{jt}=f(\bfx_{jt})+\omega_{jt}+\epsilon_{jt}\)`

--

### Substitute unobservable productivity `\(\omega_{jt}\)` with `\(h(\bfx_{jt-1},d_{t-1})\)`, real factor price `\(d_{t-1}\)`.

--

### If `\(d_{t}\)` is constant, conditional (on IV, usually `\(\bfx_{jt-1}\)`) expectation of `\(y_{jt}\)` cannot separately identify parameters of `\(f, h\)`.
--

### This also holds for dynamic panel estimators. 

--

### Monte Carlo: Even when `\(d_{t}\)` varies, identification is weak, not usable.
---
class: inverse
Production function

&lt;!--$$`
Y_{jt}=F^{0}(K_{jt}, L_{jt}, M_{jt})e^{\omega_{jt}+\epsilon_{jt}}.
`$$--&gt;

`\begin{aligned}
Y_{jt}
&amp;=F^{0}(K_{jt}, L_{jt}, M_{jt})e^{\omega_{jt}+\epsilon_{jt}},\\
y_{jt}
&amp;=
f^{0}\left(\bfx_{jt}\right)+\omega_{jt}+\epsilon_{jt},\\
&amp;=
f^{0}\left(\bfx_{jt}\right)-f^{0}_{M}\left(\bfx_{jt}\right)-\E[\epsilon_{jt}]+d_{t}+\epsilon_{jt}.
\end{aligned}`

where we used price-taking expected profit maximization FOC

`\begin{equation*}
\max_{M_{jt}}
\;\;\; \E\left[P_{jt}F^{0}(K_{jt}, L_{jt}, M_{jt})e^{\omega_{jt}+\epsilon_{jt}}\right]-\rho_{jt}M_{jt}
\end{equation*}`

`\begin{equation*}
\frac{\partial F^{0}\left(\bfx_{jt}\right)e^{\omega_{jt}}\E[e^{\epsilon_{jt}}]}{\partial M_{jt}}=\frac{\rho_{jt}}{P_{jt}}.
\end{equation*}`

`\begin{equation*}
f^{0}_{M}\left(\bfx_{jt}\right)+\omega_{jt}+\E[\epsilon_{jt}]=\underbrace{\ln\rho_{t}-p_{t}}_{\equiv d_{t}},
\quad 
f^{0}_{M}\left(\bfx_{jt}\right)=\ln\frac{\partial F^{0}\left(\bfx_{jt}\right)}{\partial M_{jt}},
\end{equation*}`
so one can solve for `\(\omega_{jt}\)` ("invert")
`\begin{equation*}
\omega_{jt}
=
-f^{0}_{M}\left(\bfx_{jt}\right)-\E[\epsilon_{jt}]+d_{t}.
\end{equation*}`
---
class: inverse
`\begin{aligned}
\omega_{jt}
&amp;=
\underbrace{-f^{0}_{M}\left(\bfx_{jt}\right)-\E[\epsilon_{jt}]}_{\equiv M^{-1}\left(\bfx_{jt}\right)}+d_{t},\\
y_{jt}
&amp;=
f^{0}\left(\bfx_{jt}\right)+\omega_{jt}+\epsilon_{jt},\\
&amp;=
\underbrace{f^{0}\left(\bfx_{jt}\right)+M^{-1}\left(\bfx_{jt}\right)}_{\equiv \phi_{G}\left(\bfx_{jt}\right)}+d_{t}+\epsilon_{jt}.\quad (\phi_{G}+d_{t}=\phi \ \mathrm{of\ DLW})
\end{aligned}`

Using `\(\phi_{G}\)`
`\begin{aligned}
\omega_{jt}
&amp;=
f^{0}\left(\bfx_{jt}\right)+M^{-1}\left(\bfx_{jt}\right)+d_{t}-f^{0}\left(\bfx_{jt}\right),\\
&amp;=
\phi_{G}\left(\bfx_{jt}\right)+d_{t}-f^{0}\left(\bfx_{jt}\right),\\
\omega_{jt-1}
&amp;=
\phi_{G}\left(\bfx_{jt-1}\right)+d_{t-1}-f^{0}\left(\bfx_{jt-1}\right).
\end{aligned}`

First-order Markov ass. gives
`\begin{aligned}
\omega_{jt}
&amp;=
h^{0}\left(\omega_{jt-1}\right)+\xi_{jt}, \quad (h^{0}=g \ \mathrm{of\ DLW})\\
&amp;=
h^{0}\left\{\phi_{G}\left(\bfx_{jt-1}\right)+d_{t-1}-f^{0}\left(\bfx_{jt-1}\right)\right\}+\xi_{jt}.
\end{aligned}`
So, with price-taking profit max FOC and FO Markov, we get
`\begin{equation*}
y_{jt}=f^{0}\left(\bfx_{jt}\right)+h^{0}\left\{\phi_{G}\left(\bfx_{jt-1}\right)+d_{t-1}-f^{0}\left(\bfx_{jt-1}\right)\right\}+\xi_{jt}+\epsilon_{jt}.
\end{equation*}`
---
class: inverse
Theorem 1: Nonidentification of parameters
### If real factor price `\(d_{t}=d\)` for all `\(t\)`,  model+data `\(\rightarrow\)` non unique parameter sets

* Conditional expectation (conditioned on IVs `\(\Gamma_{jt}\)`) of `\(y_{jt}=f^{0}(\bfx_{jt})+h^{0}(z)+\xi_{jt}+\epsilon_{jt}\)` can be expressed as `\(\E\left[f^{0}(\bfx_{jt})\left|\right.\Gamma_{jt}\right]+\E\left[h^{0}(z)\left|\right.\Gamma_{jt}\right]\)`.  
* But, when we define new functions `\(\tilde{f}\neq f^{0}, \tilde{h}\neq h^{0}\)` by mixing a nuisance function at a constant proportion `\(a\)` for any `\(a\in(0, 1)\)` as in the below, we can show (in Theorem 1) that their conditional (on IVs) expectations are same `\(\E\left[f^{0}(\bfx_{jt})\left|\right.\Gamma_{jt}\right]+\E\left[h^{0}(z)\left|\right.\Gamma_{jt}\right]=\E\left[\tilde{f}(\bfx_{jt})\left|\right.\Gamma_{jt}\right]+\E\left[\tilde{h}(z)\left|\right.\Gamma_{jt}\right]\)`.  

`\begin{aligned}
\tilde{f}\left(\bfx_{jt}\right)&amp;\equiv (1-a)f^{0}\left(\bfx_{jt}\right)+a\phi_{G}\left(\bfx_{jt}\right),\\
\tilde{h}\left(z\right)&amp;\equiv ad+(1-a)h^{0}\left(\frac{z-ad}{1-a}\right).
\end{aligned}`

* For true functions `\(f^{0}, h^{0}\)`, there are infinitely many *wrong* functions `\(\tilde{f}, \tilde{h}\)`, or different "parameters", whose conditional expectations (of functional values) are identical (*observationally equivalent*). 
---
class: inverse
Start from here:
`\begin{equation*}
y_{jt}=f^{0}\left(\bfx_{jt}\right)+h^{0}\left\{\phi_{G}\left(\bfx_{jt-1}\right)+d_{t-1}-f^{0}\left(\bfx_{jt-1}\right)\right\}+\xi_{jt}+\epsilon_{jt}.
\end{equation*}`
Expectation conditional on IVs `\(\Gamma_{jt}=\left(k_{jt}, l_{jt}, \bfx_{jt-1}, d_{t-1}, y_{jt-1}, \cdots\right)\)`: 
`\begin{equation*}
\E\left[y_{jt}|\Gamma_{jt}\right]=\E\left[f^{0}\left(\bfx_{jt}\right)|\Gamma_{jt}\right]+h^{0}\left\{\phi_{G}\left(\bfx_{jt-1}\right)+d_{t-1}-f^{0}\left(\bfx_{jt-1}\right)\right\}.
\end{equation*}`
Note from \eqref{eq9}, we have:
`\begin{equation*}
\E\left[y_{jt}|\Gamma_{jt}\right]=\E\left[\phi_{G}\left(\bfx_{jt}\right)+d_{t}|\Gamma_{jt}\right].
\end{equation*}`
Substitute this RHS into LHS of the previous equation, and rearrange:
`\begin{equation}
\E\left[\phi_{G}\left(\bfx_{jt}\right)+d_{t}-f^{0}\left(\bfx_{jt}\right)|\Gamma_{jt}\right]=h^{0}\left\{\phi_{G}\left(\bfx_{jt-1}\right)+d_{t-1}-f^{0}\left(\bfx_{jt-1}\right)\right\}.
\tag{10}
\end{equation}`
---
class: inverse
Define `\(\tilde{f}, \tilde{h}\)` and form systematic part of `\(\tilde{y}_{jt}\)`:
`\begin{aligned}
\tilde{f}\left(\bfx_{jt}\right)&amp;+\tilde{h}\left\{\phi_{G}\left(\bfx_{jt-1}\right)+d_{t-1}-\tilde{f}\left(\bfx_{jt-1}\right)\right\}\\
&amp;=
(1-a)f^{0}\left(\bfx_{jt}\right)+a\phi_{G}\left(\bfx_{jt}\right)+ad+(1-a)h^{0}\left\{\frac{\phi_{G}\left(\bfx_{jt-1}\right)+d-\tilde{f}\left(\bfx_{jt-1}\right)-ad}{1-a}\right\}.
\end{aligned}`
Numerator of square bracket
`\begin{equation*}
\phi_{G}\left(\bfx_{jt-1}\right)+d-(1-a)f^{0}\left(\bfx_{jt-1}\right)-a\phi_{G}\left(\bfx_{jt-1}\right)-ad
=
(1-a)\left\{\phi_{G}\left(\bfx_{jt-1}\right)+d-\tilde{f}\left(\bfx_{jt-1}\right)\right\}.
\end{equation*}`
So
`\begin{equation*}
RHS
=
f^{0}\left(\bfx_{jt}\right)+a\left\{\phi_{G}\left(\bfx_{jt}\right)+d-f^{0}\left(\bfx_{jt}\right)\right\}
+(1-a)h^{0}\left\{\phi_{G}\left(\bfx_{jt-1}\right)+d-f^{0}\left(\bfx_{jt-1}\right)\right\}.
\end{equation*}`
This gives the equivalence result.
`\begin{aligned}
\E&amp;\left[\tilde{f}\left(\bfx_{jt}\right)|\Gamma_{jt}\right]+\tilde{h}\left\{\phi_{G}\left(\bfx_{jt-1}\right)+d_{t-1}-\tilde{f}\left(\bfx_{jt-1}\right)\right\}\\
&amp;\hspace{1em}=
\E\left[f^{0}\left(\bfx_{jt}\right)|\Gamma_{jt}\right]+ah^{0}\left\{\phi_{G}\left(\bfx_{jt}\right)+d-f^{0}\left(\bfx_{jt}\right)\right\}
+(1-a)h^{0}\left\{\phi_{G}\left(\bfx_{jt-1}\right)+d-f^{0}\left(\bfx_{jt-1}\right)\right\},\\
&amp;\hspace{1em}=
\E\left[f^{0}\left(\bfx_{jt}\right)|\Gamma_{jt}\right]+h^{0}\left\{\phi_{G}\left(\bfx_{jt}\right)+d-f^{0}\left(\bfx_{jt}\right)\right\}.
\end{aligned}`
---
class: inverse, middle

# Semi positive result: Nonparametric identification with profit max FOC

---
class: inverse

Price-taking profit maximization gives
`\begin{equation*}
P_{t}\frac{\partial F^{0}\left(\bfx_{jt}\right)}{\partial M_{jt}}e^{\omega_{jt}}\E[e^{\epsilon_{jt}}]=\rho_{t}.
\end{equation*}`
Noting
`\begin{equation*}
\frac{\partial \ln F^{0}\left(\bfx_{jt}\right)}{\partial m_{jt}}=
\frac{\partial \ln F^{0}\left(\bfx_{jt}\right)}{\partial F^{0}}
\frac{\partial F^{0}}{\partial M_{jt}}
\frac{\partial M_{jt}}{\partial m_{jt}}=\frac{1}{F^{0}}F^{0}_{M}M_{jt},
\end{equation*}`
where the last derivative is given by:
`\begin{equation*}
1=\frac{\partial M}{\partial M}=\frac{\partial M}{\partial m}\frac{\partial m}{\partial M}=\frac{\partial M}{\partial m}\frac{1}{M} \quad \Rightarrow \quad \frac{\partial M}{\partial m}=M,
\end{equation*}`
so, 
`\begin{equation*}
F^{0}_{M}=\frac{\partial \ln F^{0}\left(\bfx_{jt}\right)}{\partial m_{jt}}\frac{F^{0}}{M_{jt}}=f^{0}_{m}\left(\bfx_{jt}\right)\frac{F^{0}}{M_{jt}}, \quad f^{0}_{m}\left(\bfx_{jt}\right)\equiv\frac{\partial \ln F^{0}\left(\bfx_{jt}\right)}{\partial m_{jt}}.
\end{equation*}`
Plugging this in gives:
`\begin{aligned}
P_{t}f^{0}_{m}F^{0}\left(\bfx_{jt}\right)e^{\omega_{jt}}\E[e^{\epsilon_{jt}}]&amp;=\rho_{t}M_{jt}.
\end{aligned}`
---
class: inverse
Plugging this in gives:
`\begin{aligned}
P_{t}f^{0}_{m}F^{0}\left(\bfx_{jt}\right)e^{\omega_{jt}}\E[e^{\epsilon_{jt}}]&amp;=\rho_{t}M_{jt},\\
P_{t}f^{0}_{m}\underbrace{F^{0}\left(\bfx_{jt}\right)e^{\omega_{jt}}e^{\epsilon_{jt}}}_{\equiv Y_{jt}}e^{-\epsilon_{jt}}\E[e^{\epsilon_{jt}}]=\rho_{t}M_{jt},\\
f^{0}_{m}e^{-\epsilon_{jt}}\E[e^{\epsilon_{jt}}]=\frac{\rho_{t}M_{jt}}{P_{jt}Y_{jt}}\equiv S_{jt}.
\end{aligned}`
Taking logs and denoting `\(\ln E[e^{\epsilon_{jt}}]=\ln\E\)` gives:
`\begin{equation*}
s_{jt}=\underbrace{\ln f^{0}_{m}+\ln\varepsilon}_{\equiv \ln D^{\epsilon}\left(\bfx_{jt}\right)}-\epsilon_{jt}, \quad D^{\epsilon}\left(\bfx_{jt}\right)=f^{0}_{m}\left(\bfx_{jt}\right)\E[e^{\epsilon_{jt}}].
\end{equation*}`
Equivalently, *share regression* is derived:
`\begin{equation*}
s_{jt}=\ln D^{\epsilon}\left(\bfx_{jt}\right)+u^{\epsilon}_{jt}.
\tag{s}\label{sharereg}
\end{equation*}`

---
class: inverse
### Thereom 2: One can recover `\(\hat{f}_{m}\)` by using the share regression.

Given `\(\omega_{jt}, \bfx_{jt}\in\mathcal I_{jt}\)` we have `\(\E[\epsilon_{jt}|\mathcal I_{jt}]=0\)`, so regressing `\(s_{jt}\)` on a flexible function of `\(\bfx_{jt}\)` identifies `\(f^{0}_{m}\)` as a set of coefficients on the part of `\(m_{jt}\)`. 
`\begin{equation*}
\E\left.\left[s_{jt}\right|\bfx_{jt}\right]=\ln D^{\epsilon}\left(\bfx_{jt}\right).
\end{equation*}`
This identifies `\(D^{\epsilon}\)`. This gives `\(\epsilon_{jt}\)` and `\(\E=\E[e^{\epsilon_{jt}}]\)`:
`\begin{equation*}
\hat{\epsilon}_{jt}=\ln \hat{D}^{\epsilon}-s_{jt}, \quad \hat{\E}=\E\left[e^{\ln \hat{D}^{\epsilon}-s_{jt}}\right].
\end{equation*}`
This gives:
`\begin{aligned}
\ln \hat{f}_{m}
&amp;=\ln \hat{D}^{\epsilon}-\ln\hat{\E},\\
\hat{f}_{m}
&amp;=\frac{\hat{D}^{\epsilon}}{\hat{\E}}.
\end{aligned}`

---
class: inverse
### Thereom 3: Rest of `\(f\)` is recovered up to a constant if `\(\hat{f}_{m}\)` is known.

By the fundamental theorem of calculus:
`\begin{equation}
\int f^{0}_{m}\left(\bfx_{jt}\right)dm_{jt}=f^{0}\left(\bfx_{jt}\right)+\mathcal C(k_{jt}, l_{jt}), \quad \mathrm{or}\quad
f^{0}\left(\bfx_{jt}\right)=\int f^{0}_{m}\left(\bfx_{jt}\right)dm_{jt}-\mathcal C(k_{jt}, l_{jt}).
\tag{f}\label{f0identified}
\end{equation}`
where the constant of integration `\(\mathcal C\)` is allowed to depend on `\(k_{jt}, l_{jt}\)`, or `\(\mathcal C=c_{0}+c_{1}(k_{jt}, l_{jt})\)` without loss of generality. 

### If we can compute `\(c_{1}(k_{jt}, l_{jt})\)`, then we can identify `\(f^{0}\left(\bfx_{jt}\right)\)` up to a constant `\(c_{0}\)`. 

---
class: inverse
Start with the production function in logs:
`\begin{aligned}
\omega_{jt}
&amp;=
y_{jt}-f^{0}-\epsilon_{jt},\\
&amp;=
y_{jt}-\left\{\int f^{0}_{m}\left(\bfx_{jt}\right)dm_{jt}-\mathcal C(k_{jt}, l_{jt})\right\}-\epsilon_{jt}.
\end{aligned}`
Define
`\begin{aligned}
\green{\mathcal Y_{jt}}
&amp;\equiv 
y_{jt}-\int \red{f^{0}_{m}\left(\bfx_{jt}\right)}dm_{jt}-\red{\epsilon_{jt}},\\
&amp;=
\omega_{jt}-\mathcal C(k_{jt}, l_{jt}).
\end{aligned}`
We already .red[have] `\(\red{\hat{\epsilon}_{jt}}\)` and `\(\red{\hat{f}^{0}_{m}}\)`, then we can .green[get] `\(\green{\hat{\mathcal Y}_{jt}}\)`. 

Using first-order Markov on `\(\omega_{jt}\)`
`\begin{aligned}
\omega_{jt}
&amp;=
\mathcal Y_{jt}+\mathcal C(k_{jt}, l_{jt}),\\
\omega_{jt-1}
&amp;=
\mathcal Y_{jt-1}+\mathcal C(k_{jt-1}, l_{jt-1}).
\end{aligned}`
Then
`\begin{aligned}
\mathcal Y_{jt}
&amp;=
h^{0}\left\{\omega_{jt-1}\right\}+\xi_{jt}-\mathcal C(k_{jt}, l_{jt}),\\
&amp;=
h^{0}\left\{\mathcal Y_{jt-1}+\mathcal C(k_{jt-1}, l_{jt-1})\right\}+\xi_{jt}-\mathcal C(k_{jt}, l_{jt}).
\end{aligned}`
---
class: inverse
Denote `\(\Gamma_{y}=(k_{jt}, l_{jt}, k_{jt-1}, l_{jt-1}, \mathcal Y_{jt-1})\)`. Then
`\begin{equation*}
E\left.\left[\mathcal Y_{jt}\right|\Gamma_{y}\right]=-\mathcal C(k_{jt}, l_{jt})+h^{0}\left\{\mathcal Y_{jt-1}+\mathcal C(k_{jt-1}, l_{jt-1})\right\}.
\end{equation*}`

### Partial derivatives `\(c_{1}(k_{jt}, l_{jt})\)` of integration `\(\mathcal C\)` are (negative of) coefficients on `\(k_{jt}, l_{jt}\)` conditional on `\(\mathcal Y_{jt-1}, k_{jt-1}, l_{jt-1}\)`.  

* We cannot identify `\(c_{0}\)` part.  
* Using this in \eqref{f0identified} gives `\(\hat{f}^{0}\)` up to a constant.  

### `\(\hat{\omega}_{jt}-c_{0}=\hat{\mathcal Y}_{jt}+\hat{c}_{1}(k_{jt}, l_{jt})\)` is estimable. One can measure productivity differences `\(\hat{\omega}_{jt}-\hat{\omega}_{i't}\)` or `\(\hat{\omega}_{jt}-\hat{\omega}_{it'}\)`. 

---
class: inverse

Cool but *semi* positive results. 

Monopolist profit maximization
`\begin{aligned}
\rho_{t}
&amp;=
\left(\red{P'_{t}F^{0}\left(\bfx_{jt}\right)}+P_{t}\frac{\partial F^{0}\left(\bfx_{jt}\right)}{\partial M_{jt}}\right)e^{\omega_{jt}}\E[e^{\epsilon_{jt}}],\\
&amp;=
\left(\frac{\partial P}{\partial Y_{jt}}\frac{F^{0}\left(\bfx_{jt}\right)}{P_{t}}+f^{0}_{m}\left(\bfx_{jt}\right)\frac{F^{0}}{M_{jt}}\right)P_{t}e^{\omega_{jt}}\E[e^{\epsilon_{jt}}],\\
&amp;=
\left(-\eta^{-1}(\bfx_{jt}, \omega_{jt}, \epsilon_{jt})+f^{0}_{m}\left(\bfx_{jt}\right)\frac{F^{0}e^{\omega_{jt}}\E[e^{\epsilon_{jt}}]}{M_{jt}}\right)P_{t},\\
S_{jt}
&amp;=
-\eta^{-1}(\bfx_{jt}, \omega_{jt}, \epsilon_{jt})\frac{M_{jt}}{Y_{jt}}+f^{0}_{m}\left(\bfx_{jt}\right),\\
&amp;=
-\frac{\eta^{-1}(\red{\bfx_{jt}}, \omega_{jt}, \epsilon_{jt})}{\beta^{M}}+f^{0}_{m}\left(\red{\bfx_{jt}}\right), \quad \beta^{M}\equiv\frac{M_{jt}}{Y_{jt}}.\\
\end{aligned}`

`\(\red{\bfx_{jt}}\)` appears both in `\(\eta\)` and `\(f^{0}_{m}\)` and the two functions cannot be separately nonparametrically identified with the share regression. 

* If we assume an iso-elastic demand function for the output, then elasticity does not depend on `\(\bfx_{jt}\)` and can recover production function parameters. GNR does so in the appendix. 

---
class: inverse, middle

# Extensions and empirical exercises

---
class: inverse

* 
Doraszelski and Jaumandreu (2013) achieves identification of Cobb-Douglas using FOCs. Doraszelski and Jaumandreu (2018) extends it to non-Hicks neutral, CES production.  
* GNR can incoporate firm fixed effects but at the cost of loss of efficiency due to differencing.  

### Using Chile and Columbia data, OLS estimates are overestimated than GNR estimates in more flexible inputs (materials).  
### Less flexible input elasticities are underestimated with OLS. 
### Productivity is more dispersed under GNR estimates. 

---
class: inverse




[1] U. Doraszelski and J. Jaumandreu. "R&amp; D and productivity: Estimating endogenous productivity". In:
_The Review of Economic Studies_ 80.4 (3. 2013), pp. 1338-1383. ISSN: 0034-6527. DOI:
[10.1093/restud/rdt011](https://doi.org/10.1093%2Frestud%2Frdt011). eprint:
https://academic.oup.com/restud/article-pdf/80/4/1338/18386585/rdt011.pdf. URL:
[https://doi.org/10.1093/restud/rdt011](https://doi.org/10.1093/restud/rdt011).

[2] D. A. Ackerberg, K. Caves, and G. Frazer. "Identification properties of recent production function
estimators". In: _Econometrica_ 83.6 (2015), pp. 2411-2451. DOI:
[10.3982/ECTA13408](https://doi.org/10.3982%2FECTA13408). eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA13408. URL:
[https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA13408](https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA13408).

[3] U. Doraszelski and J. Jaumandreu. "Measuring the bias of technological change". In: _Journal of
Political Economy_ 126.3 (2018), pp. 1027-1084. DOI:
[10.1086/697204](https://doi.org/10.1086%2F697204). eprint:

https://doi.org/10.1086/697204 . URL: [ https://doi.org/10.1086/697204 ](
https://doi.org/10.1086/697204 ).

[4] A. Gandhi, S. Navarro, and D. A. Rivers. "On the identification of gross output production
functions". In: _Journal of Political Economy_ 128.8 (2020), pp. 2973-3016. DOI:
[10.1086/707736](https://doi.org/10.1086%2F707736). eprint: https://doi.org/10.1086/707736 . URL: [
https://doi.org/10.1086/707736 ]( https://doi.org/10.1086/707736 ).
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
